{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Text Segmentation\n",
    "\n",
    "Text Segmentation is the process of transforming text into meaningful units. These units can be words, sentences or different topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/manishanker.talusani/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "text = \"CODE is founded by Mr. Bachem. Studying at CODE will be unlike any other higher education experience. Our intensive, interdisciplinary bachelor’s programs are designed to dramatically improve the way you work and to prepare you for the reality of tomorrow’s workplace.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE is founded by Mr. Bachem.', 'Studying at CODE will be unlike any other higher education experience.', 'Our intensive, interdisciplinary bachelor’s programs are designed to dramatically improve the way you work and to prepare you for the reality of tomorrow’s workplace.']\n"
     ]
    }
   ],
   "source": [
    "# split it into sentences\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE', 'is', 'founded', 'by', 'Mr.', 'Bachem', '.', 'Studying', 'at', 'CODE', 'will', 'be', 'unlike', 'any', 'other', 'higher', 'education', 'experience', '.', 'Our', 'intensive', ',', 'interdisciplinary', 'bachelor', '’', 's', 'programs', 'are', 'designed', 'to', 'dramatically', 'improve', 'the', 'way', 'you', 'work', 'and', 'to', 'prepare', 'you', 'for', 'the', 'reality', 'of', 'tomorrow', '’', 's', 'workplace', '.']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"beneath the extraodrinary staircase...\"\n",
    "\n",
    "tokenize= sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Stop Words & Word Segmentation\n",
    "Also part of Natural Language are words that are basically useless, which are referred to as \"stop words\". Since we dont want that these words extend our processing time or take up unnecessary space in our database, we will remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manishanker.talusani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words from text\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"CODE is founded by Mr. Bachem. Studying at CODE will be unlike \n",
    "any other higher education experience. Our intensive, interdisciplinary \n",
    "bachelor’s programs are designed to dramatically improve the way you work \n",
    "and to prepare you for the reality of tomorrow’s workplace.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the stop words we will use\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the text for stop words\n",
    "filtered_sentence = [w for w in tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE', 'is', 'founded', 'by', 'Mr.', 'Bachem', '.', 'Studying', 'at', 'CODE', 'will', 'be', 'unlike', 'any', 'other', 'higher', 'education', 'experience', '.', 'Our', 'intensive', ',', 'interdisciplinary', 'bachelor', '’', 's', 'programs', 'are', 'designed', 'to', 'dramatically', 'improve', 'the', 'way', 'you', 'work', 'and', 'to', 'prepare', 'you', 'for', 'the', 'reality', 'of', 'tomorrow', '’', 's', 'workplace', '.']\n"
     ]
    }
   ],
   "source": [
    "# show just the tokenized text\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE', 'founded', 'Mr.', 'Bachem', '.', 'Studying', 'CODE', 'unlike', 'higher', 'education', 'experience', '.', 'Our', 'intensive', ',', 'interdisciplinary', 'bachelor', '’', 'programs', 'designed', 'dramatically', 'improve', 'way', 'work', 'prepare', 'reality', 'tomorrow', '’', 'workplace', '.']\n"
     ]
    }
   ],
   "source": [
    "# show filtered tokenized text\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming single words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"ride\",\"riding\", \"rider\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride\n",
      "ride\n",
      "rider\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"\"\"CODE is a newly founded private university of applied sciences that is embedded into the vibrant \n",
    "network of Berlin's digital economy.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\n",
      "is\n",
      "a\n",
      "newli\n",
      "found\n",
      "privat\n",
      "univers\n",
      "of\n",
      "appli\n",
      "scienc\n",
      "that\n",
      "is\n",
      "embed\n",
      "into\n",
      "the\n",
      "vibrant\n",
      "network\n",
      "of\n",
      "berlin\n",
      "'s\n",
      "digit\n",
      "economi\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IV. Parsing (Speech Tagging & Chunking)\n",
    "\n",
    "## 1. Speech Tagging\n",
    "\n",
    "Speech Tagging in NLTK is the process of labeling words in a sentence as nouns, adjectives, verbs and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, NLTK provides us with a sentence tokenizer called the \"PunktSentenceTokenizer\", which is a un-supervised ML algorithm that can be trained on any text corpus you wish to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/manishanker.talusani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/manishanker.talusani/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# using novels by chesterton\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import gutenberg\n",
    "test = gutenberg.raw(\"chesterton-ball.txt\")\n",
    "train = gutenberg.raw(\"chesterton-brown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train)\n",
    "# tokenize chesterton ball\n",
    "tokenized = custom_sent_tokenizer.tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'IN'), ('The', 'DT'), ('Ball', 'NNP'), ('and', 'CC'), ('The', 'DT'), ('Cross', 'NNP'), ('by', 'IN'), ('G.K', 'NNP'), ('.', '.')]\n",
      "[('Chesterton', 'NNP'), ('1909', 'CD'), (']', 'NN'), ('I', 'PRP'), ('.', '.')]\n",
      "[('A', 'DT'), ('DISCUSSION', 'NNP'), ('SOMEWHAT', 'NNP'), ('IN', 'NNP'), ('THE', 'NNP'), ('AIR', 'NNP'), ('The', 'DT'), ('flying', 'VBG'), ('ship', 'NN'), ('of', 'IN'), ('Professor', 'NNP'), ('Lucifer', 'NNP'), ('sang', 'VBD'), ('through', 'IN'), ('the', 'DT'), ('skies', 'NNS'), ('like', 'IN'), ('a', 'DT'), ('silver', 'NN'), ('arrow', 'NN'), (';', ':'), ('the', 'DT'), ('bleak', 'JJ'), ('white', 'JJ'), ('steel', 'NN'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('gleaming', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('bleak', 'JJ'), ('blue', 'JJ'), ('emptiness', 'NN'), ('of', 'IN'), ('the', 'DT'), ('evening', 'NN'), ('.', '.')]\n",
      "[('That', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('far', 'RB'), ('above', 'IN'), ('the', 'DT'), ('earth', 'NN'), ('was', 'VBD'), ('no', 'DT'), ('expression', 'NN'), ('for', 'IN'), ('it', 'PRP'), (';', ':'), ('to', 'TO'), ('the', 'DT'), ('two', 'CD'), ('men', 'NNS'), ('in', 'IN'), ('it', 'PRP'), (',', ','), ('it', 'PRP'), ('seemed', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('far', 'RB'), ('above', 'IN'), ('the', 'DT'), ('stars', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('professor', 'NN'), ('had', 'VBD'), ('himself', 'PRP'), ('invented', 'VBN'), ('the', 'DT'), ('flying', 'VBG'), ('machine', 'NN'), (',', ','), ('and', 'CC'), ('had', 'VBD'), ('also', 'RB'), ('invented', 'VBN'), ('nearly', 'RB'), ('everything', 'NN'), ('in', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
      "[('Every', 'DT'), ('sort', 'NN'), ('of', 'IN'), ('tool', 'NN'), ('or', 'CC'), ('apparatus', 'NN'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), (',', ','), ('to', 'TO'), ('the', 'DT'), ('full', 'JJ'), (',', ','), ('that', 'IN'), ('fantastic', 'JJ'), ('and', 'CC'), ('distorted', 'JJ'), ('look', 'NN'), ('which', 'WDT'), ('belongs', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('miracles', 'NNS'), ('of', 'IN'), ('science', 'NN'), ('.', '.')]\n",
      "[('For', 'IN'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('science', 'NN'), ('and', 'CC'), ('evolution', 'NN'), ('is', 'VBZ'), ('far', 'RB'), ('more', 'JJR'), ('nameless', 'JJ'), ('and', 'CC'), ('elusive', 'JJ'), ('and', 'CC'), ('like', 'IN'), ('a', 'DT'), ('dream', 'NN'), ('than', 'IN'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('poetry', 'NN'), ('and', 'CC'), ('religion', 'NN'), (';', ':'), ('since', 'IN'), ('in', 'IN'), ('the', 'DT'), ('latter', 'JJ'), ('images', 'NNS'), ('and', 'CC'), ('ideas', 'NNS'), ('remain', 'VBP'), ('themselves', 'PRP'), ('eternally', 'RB'), (',', ','), ('while', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('whole', 'JJ'), ('idea', 'NN'), ('of', 'IN'), ('evolution', 'NN'), ('that', 'IN'), ('identities', 'VBZ'), ('melt', 'FW'), ('into', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('as', 'IN'), ('they', 'PRP'), ('do', 'VBP'), ('in', 'IN'), ('a', 'DT'), ('nightmare', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def tag_text():\n",
    "    try:\n",
    "        for i in tokenized[:7]:\n",
    "            actual_words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(actual_words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "tag_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking Text\n",
    "\n",
    "Chunking is the process of grouping words into more meaningful junks than just the speech tags. This can be things such as \"noun phrases\" or \"verb phrases\". With chunking you can get a parse tree.\n",
    "\n",
    "We will search for chunks that correspond to individual noun phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pre-tagged text out of simplicity\n",
    "text = [(\"the\", \"DT\"), (\"huge\", \"JJ\"), (\"german\", \"JJ\"), (\"Rottweiler\", \"NN\"), \n",
    "        (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT huge/JJ german/JJ Rottweiler/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# define a noun-phrase as:\n",
    "# np = determiner + adjective + singular noun\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "\n",
    "# apply grammar to regexparser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# do the chunking\n",
    "result = cp.parse(text) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Sentiment Analysis using Keras\n",
    "\n",
    "Blogpost:\n",
    "https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/1c/3ac472009a5c54ae7ec5a3294520ca36d1908cd1e5cf3e3fd923f9b7b31f/tensorflow-1.13.1-cp37-cp37m-macosx_10_11_x86_64.whl (73.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 73.6MB 390kB/s ta 0:00:011    94% |██████████████████████████████▏ | 69.3MB 12.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 10.0MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /Users/manishanker.talusani/.b/lib/python3.7/site-packages (from tensorflow) (1.0.7)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4f/b4ab63001f81ccc3ef754965070000f056b2c13fadf93ef46ac231e3086d/protobuf-3.7.0-cp37-cp37m-macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 5.8MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/manishanker.talusani/.b/lib/python3.7/site-packages (from tensorflow) (0.33.1)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 5.5MB/s ta 0:00:011   7% |██▌                             | 245kB 9.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /Users/manishanker.talusani/.b/lib/python3.7/site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/manishanker.talusani/.b/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/manishanker.talusani/.b/lib/python3.7/site-packages (from tensorflow) (1.16.2)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/33/c0561fe7c5e235325255f46c08bd3d07f2c80824feb22d057328eff1f8b7/grpcio-1.19.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 7.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 7.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: h5py in /Users/manishanker.talusani/.b/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/manishanker.talusani/.b/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 18.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 19.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl (107kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 12.0MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: absl-py, gast, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/manishanker.talusani/Library/Caches/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/manishanker.talusani/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/manishanker.talusani/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built absl-py gast termcolor\n",
      "Installing collected packages: absl-py, gast, astor, protobuf, grpcio, markdown, werkzeug, tensorboard, pbr, mock, tensorflow-estimator, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 markdown-3.0.1 mock-2.0.0 pbr-5.1.3 protobuf-3.7.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 16s 1us/step\n",
      "WARNING:tensorflow:From /Users/manishanker.talusani/.b/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/manishanker.talusani/.b/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                500050    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 505,201\n",
      "Trainable params: 505,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/manishanker.talusani/.b/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 10s 245us/step - loss: 0.4051 - acc: 0.8210 - val_loss: 0.2638 - val_acc: 0.8942\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 0.2120 - acc: 0.9193 - val_loss: 0.2590 - val_acc: 0.8938\n",
      "Test-Accuracy: 0.8940000012516975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "def vectorize(sequences, dimension = 10000):\n",
    " results = np.zeros((len(sequences), dimension))\n",
    " for i, sequence in enumerate(sequences):\n",
    "  results[i, sequence] = 1\n",
    " return results\n",
    " \n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")\n",
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]\n",
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "# compiling the model\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n",
    "results = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 2,\n",
    " batch_size = 500,\n",
    " validation_data = (test_x, test_y)\n",
    ")\n",
    "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a IMDB Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 6s 4us/step\n",
      "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "Label: 1 , which means positive.\n"
     ]
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
    "print(decoded)\n",
    "print(\"Label:\", targets[0], \", which means positive.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
